{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1, .md-content__button { display: none; } Postgres ML Simple machine learning with PostgreSQL Train and deploy models to make online predictions using only SQL, with an open source extension for Postgres. Manage your projects and visualize datasets using the built in dashboard. The dashboard makes it easy to compare different algorithms or hyperparaters across models and datasets. See it in action \u2014 demo.postgresml.org What's in the box \u00b6 See the documentation for a complete list of functionality . All your favorite algorithms \u00b6 Whether you need a simple linear regression, or extreme gradient boosting, we've included support for all classification and regression algorithms in Scikit Learn and XGBoost with no extra configuration. Managed model deployements \u00b6 Models can be periodically retrained and automatically promoted to production depending on their key metric. Rollback capability is provided to ensure that you're always able to serve the highest quality predictions, along with historical logs of all deployments for long term study. Online and offline support \u00b6 Predictions are served via a standard Postgres connection to ensure that your core apps can always access both your data and your models in real time. Pure SQL workflows also enable batch predictions to cache results in native Postgres tables for lookup. Instant visualizations \u00b6 Run standard analysis on your datasets to detect outliers, bimodal distributions, feature correlation, and other common data visualizations on your datasets. Everything is cataloged in the dashboard for easy reference. Hyperparameter search \u00b6 Use either grid or random searches with cross validation on your training set to discover the most important knobs to tweak on your favorite algorithm. The performance of Postgres \u00b6 Since your data never leaves the database, you retain the speed, reliability and security you expect in your foundational stateful services. Leverage your existing infrastructure and expertise to deliver new capabilities. Open source \u00b6 We're building on the shoulders of giants. These machine learning libraries and Postgres have recieved extensive academic and industry use, and we'll continue their tradition to build with the community. Licensed under MIT. Quick Start \u00b6 1) Clone this repo: $ git clone git@github.com:postgresml/postgresml.git 2) Start dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running: $ cd postgresml && docker-compose up 3) Connect to Postgres in the container with PostgresML installed: $ psql postgres://postgres@localhost:5433/pgml_development 4) Validate your installation: pgml_development =# SELECT pgml . version (); version --------- 0 . 8 . 1 ( 1 row ) See the documentation for a complete guide to working with PostgresML .","title":"Home"},{"location":"#whats-in-the-box","text":"See the documentation for a complete list of functionality .","title":"What's in the box"},{"location":"#all-your-favorite-algorithms","text":"Whether you need a simple linear regression, or extreme gradient boosting, we've included support for all classification and regression algorithms in Scikit Learn and XGBoost with no extra configuration.","title":"All your favorite algorithms"},{"location":"#managed-model-deployements","text":"Models can be periodically retrained and automatically promoted to production depending on their key metric. Rollback capability is provided to ensure that you're always able to serve the highest quality predictions, along with historical logs of all deployments for long term study.","title":"Managed model deployements"},{"location":"#online-and-offline-support","text":"Predictions are served via a standard Postgres connection to ensure that your core apps can always access both your data and your models in real time. Pure SQL workflows also enable batch predictions to cache results in native Postgres tables for lookup.","title":"Online and offline support"},{"location":"#instant-visualizations","text":"Run standard analysis on your datasets to detect outliers, bimodal distributions, feature correlation, and other common data visualizations on your datasets. Everything is cataloged in the dashboard for easy reference.","title":"Instant visualizations"},{"location":"#hyperparameter-search","text":"Use either grid or random searches with cross validation on your training set to discover the most important knobs to tweak on your favorite algorithm.","title":"Hyperparameter search"},{"location":"#the-performance-of-postgres","text":"Since your data never leaves the database, you retain the speed, reliability and security you expect in your foundational stateful services. Leverage your existing infrastructure and expertise to deliver new capabilities.","title":"The performance of Postgres"},{"location":"#open-source","text":"We're building on the shoulders of giants. These machine learning libraries and Postgres have recieved extensive academic and industry use, and we'll continue their tradition to build with the community. Licensed under MIT.","title":"Open source"},{"location":"#quick-start","text":"1) Clone this repo: $ git clone git@github.com:postgresml/postgresml.git 2) Start dockerized services. PostgresML will run on port 5433, just in case you already have Postgres running: $ cd postgresml && docker-compose up 3) Connect to Postgres in the container with PostgresML installed: $ psql postgres://postgres@localhost:5433/pgml_development 4) Validate your installation: pgml_development =# SELECT pgml . version (); version --------- 0 . 8 . 1 ( 1 row ) See the documentation for a complete guide to working with PostgresML .","title":"Quick Start"},{"location":"about/faq/","text":"FAQ \u00b6 How far can this scale? Petabyte-sized Postgres deployments are documented in production since at least 2008, and recent patches have enabled working beyond exabyte and up to the yotabyte scale. Machine learning models can be horizontally scaled using standard Postgres replicas. How reliable can this be? Postgres is widely considered mission critical, and some of the most reliable technology in any modern stack. PostgresML allows an infrastructure organization to leverage pre-existing best practices to deploy machine learning into production with less risk and effort than other systems. For example, model backup and recovery happens automatically alongside normal Postgres data backup. How good are the models? Model quality is often a tradeoff between compute resources and incremental quality improvements. Sometimes a few thousands training examples and an off the shelf algorithm can deliver significant business value after a few seconds of training. PostgresML allows stakeholders to choose several different algorithms to get the most bang for the buck, or invest in more computationally intensive techniques as necessary. In addition, PostgresML automatically applies best practices for data cleaning like imputing missing values by default and normalizing data to prevent common problems in production. PostgresML doesn't help with reformulating a business problem into a machine learning problem. Like most things in life, the ultimate in quality will be a concerted effort of experts working over time. PostgresML is intended to establish successful patterns for those experts to collaborate around while leveraging the expertise of open source and research communities. Is PostgresML fast? Colocating the compute with the data inside the database removes one of the most common latency bottlenecks in the ML stack, which is the (de)serialization of data between stores and services across the wire. Modern versions of Postgres also support automatic query parrellization across multiple workers to further minimize latency in large batch workloads. Finally, PostgresML will utilize GPU compute if both the algorithm and hardware support it, although it is currently rare in practice for production databases to have GPUs. We're working on benchmarks .","title":"FAQ"},{"location":"about/faq/#faq","text":"How far can this scale? Petabyte-sized Postgres deployments are documented in production since at least 2008, and recent patches have enabled working beyond exabyte and up to the yotabyte scale. Machine learning models can be horizontally scaled using standard Postgres replicas. How reliable can this be? Postgres is widely considered mission critical, and some of the most reliable technology in any modern stack. PostgresML allows an infrastructure organization to leverage pre-existing best practices to deploy machine learning into production with less risk and effort than other systems. For example, model backup and recovery happens automatically alongside normal Postgres data backup. How good are the models? Model quality is often a tradeoff between compute resources and incremental quality improvements. Sometimes a few thousands training examples and an off the shelf algorithm can deliver significant business value after a few seconds of training. PostgresML allows stakeholders to choose several different algorithms to get the most bang for the buck, or invest in more computationally intensive techniques as necessary. In addition, PostgresML automatically applies best practices for data cleaning like imputing missing values by default and normalizing data to prevent common problems in production. PostgresML doesn't help with reformulating a business problem into a machine learning problem. Like most things in life, the ultimate in quality will be a concerted effort of experts working over time. PostgresML is intended to establish successful patterns for those experts to collaborate around while leveraging the expertise of open source and research communities. Is PostgresML fast? Colocating the compute with the data inside the database removes one of the most common latency bottlenecks in the ML stack, which is the (de)serialization of data between stores and services across the wire. Modern versions of Postgres also support automatic query parrellization across multiple workers to further minimize latency in large batch workloads. Finally, PostgresML will utilize GPU compute if both the algorithm and hardware support it, although it is currently rare in practice for production databases to have GPUs. We're working on benchmarks .","title":"FAQ"},{"location":"about/license/","text":"Copyright \u00a9 2022 PostgresML Team Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"about/motivation/","text":"Motivation \u00b6 Deploying machine learning models into existing applications is not straight forward. It involves operating new services, which need to be written in specialized languages with libraries outside of the experience of many software engineers. Those services tend to be architected around specialized datastores and hardware that requires additional management and know how. Data access needs to be secure across production and development environments without impeding productivity. This complexity pushes risks and costs beyond acceptable trade off limits for many otherwise valuable use cases. PostgresML makes ML simple by moving the code to your data, rather than copying the data all over the place. You train models using simple SQL commands, and you get the predictions in your apps via a mechanism you're already using: a query over a standard Postgres connection. Our goal is that anyone with a basic understanding of SQL should be able to build, deploy and maintain machine learning models in production, while receiving the benefits of a high performance machine learning platform. Ultimately, PostgresML aims to be the easiest, safest and fastest way to gain value from machine learning.","title":"Motivation"},{"location":"about/motivation/#motivation","text":"Deploying machine learning models into existing applications is not straight forward. It involves operating new services, which need to be written in specialized languages with libraries outside of the experience of many software engineers. Those services tend to be architected around specialized datastores and hardware that requires additional management and know how. Data access needs to be secure across production and development environments without impeding productivity. This complexity pushes risks and costs beyond acceptable trade off limits for many otherwise valuable use cases. PostgresML makes ML simple by moving the code to your data, rather than copying the data all over the place. You train models using simple SQL commands, and you get the predictions in your apps via a mechanism you're already using: a query over a standard Postgres connection. Our goal is that anyone with a basic understanding of SQL should be able to build, deploy and maintain machine learning models in production, while receiving the benefits of a high performance machine learning platform. Ultimately, PostgresML aims to be the easiest, safest and fastest way to gain value from machine learning.","title":"Motivation"},{"location":"about/roadmap/","text":"Roadmap \u00b6 This project is currently a proof of concept. Some important features, which we are currently thinking about or working on, are listed below. Production deployment \u00b6 Most companies that use PostgreSQL in production do so using managed services like AWS RDS, Digital Ocean, Azure, etc. Those services do not allow running custom extensions, so we have to run PostgresML directly on VMs, e.g. EC2, droplets, etc. The idea here is to replicate production data directly from Postgres and make it available in real-time to PostgresML. We're considering solutions like logical replication for small to mid-size databases, and Debezium for multi-TB deployments. Model management dashboard \u00b6 A good looking and useful UI goes a long way. A dashboard similar to existing solutions like MLFlow or AWS SageMaker will make the experience of working with PostgresML as pleasant as possible. Data explorer \u00b6 A data explorer allows anyone to browse the dataset in production and to find useful tables and features to build effective machine learning models. More algorithms \u00b6 Scikit-Learn is a good start, but we're also thinking about including Tensorflow, Pytorch, and many more useful models. Scheduled training \u00b6 In applications where data changes often, it's useful to retrain the models automatically on a schedule, e.g. every day, every week, etc.","title":"Roadmap"},{"location":"about/roadmap/#roadmap","text":"This project is currently a proof of concept. Some important features, which we are currently thinking about or working on, are listed below.","title":"Roadmap"},{"location":"about/roadmap/#production-deployment","text":"Most companies that use PostgreSQL in production do so using managed services like AWS RDS, Digital Ocean, Azure, etc. Those services do not allow running custom extensions, so we have to run PostgresML directly on VMs, e.g. EC2, droplets, etc. The idea here is to replicate production data directly from Postgres and make it available in real-time to PostgresML. We're considering solutions like logical replication for small to mid-size databases, and Debezium for multi-TB deployments.","title":"Production deployment"},{"location":"about/roadmap/#model-management-dashboard","text":"A good looking and useful UI goes a long way. A dashboard similar to existing solutions like MLFlow or AWS SageMaker will make the experience of working with PostgresML as pleasant as possible.","title":"Model management dashboard"},{"location":"about/roadmap/#data-explorer","text":"A data explorer allows anyone to browse the dataset in production and to find useful tables and features to build effective machine learning models.","title":"Data explorer"},{"location":"about/roadmap/#more-algorithms","text":"Scikit-Learn is a good start, but we're also thinking about including Tensorflow, Pytorch, and many more useful models.","title":"More algorithms"},{"location":"about/roadmap/#scheduled-training","text":"In applications where data changes often, it's useful to retrain the models automatically on a schedule, e.g. every day, every week, etc.","title":"Scheduled training"},{"location":"development/overview/","text":"Contributing \u00b6 General \u00b6 Use unix line endings Setup your development environment \u00b6 1) Install pyenv for your system to use the correct version of python specified in .python-version . Make sure your $PATH now includes ~/.pyenv/bin && ~/.pyenv/shims . 2) Install pyenv-virtualenv to isolate project dependencies which keeps requirements.txt clean and frozen. 3) Install the version of python listed in .python-version : $ pyenv install 3.10.4 4) Create the virtual env: $ pyenv virtualenv 3.10.4 pgml-admin 5) Install the dependencies: $ pip install -r requirements.txt 6) If you ever add new dependencies, freeze them: $ pip freeze > requirements.txt 7) Make sure requirements.txt has no changes, which indicates your virtual environment is setup correctly. $ git status 8) setup your .env $ cp .env.TEMPLATE .env $ nano .env 9) Run the server $ ./manage.py runserver Maintain your development database \u00b6 How to reset your local database: $ psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"create schema pgml\" postgres://postgres@127.0.0.1:5433/pgml_development Follow the installation instructions to create a local working Postgres environment, then install the pgml-extension from the git repository: cd pgml-extension sudo python3 setup.py install Run the tests from the root of the repo: cd pgml-extension ON_ERROR_STOP=1 psql -f sql/test.sql postgres://postgres@127.0.0.1:5433/pgml_development One liner: cd pgml-extension; sudo /bin/pip3 install .; psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"create schema pgml\" postgres://postgres@127.0.0.1:5433/pgml_development; ON_ERROR_STOP=1 psql -f sql/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development; cd .. Make sure to run it exactly like this, from the root directory of the repo. Update documentation \u00b6 mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Overview"},{"location":"development/overview/#contributing","text":"","title":"Contributing"},{"location":"development/overview/#general","text":"Use unix line endings","title":"General"},{"location":"development/overview/#setup-your-development-environment","text":"1) Install pyenv for your system to use the correct version of python specified in .python-version . Make sure your $PATH now includes ~/.pyenv/bin && ~/.pyenv/shims . 2) Install pyenv-virtualenv to isolate project dependencies which keeps requirements.txt clean and frozen. 3) Install the version of python listed in .python-version : $ pyenv install 3.10.4 4) Create the virtual env: $ pyenv virtualenv 3.10.4 pgml-admin 5) Install the dependencies: $ pip install -r requirements.txt 6) If you ever add new dependencies, freeze them: $ pip freeze > requirements.txt 7) Make sure requirements.txt has no changes, which indicates your virtual environment is setup correctly. $ git status 8) setup your .env $ cp .env.TEMPLATE .env $ nano .env 9) Run the server $ ./manage.py runserver","title":"Setup your development environment"},{"location":"development/overview/#maintain-your-development-database","text":"How to reset your local database: $ psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"create schema pgml\" postgres://postgres@127.0.0.1:5433/pgml_development Follow the installation instructions to create a local working Postgres environment, then install the pgml-extension from the git repository: cd pgml-extension sudo python3 setup.py install Run the tests from the root of the repo: cd pgml-extension ON_ERROR_STOP=1 psql -f sql/test.sql postgres://postgres@127.0.0.1:5433/pgml_development One liner: cd pgml-extension; sudo /bin/pip3 install .; psql -c \"DROP DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"CREATE DATABASE pgml_development\" postgres://postgres@127.0.0.1:5433/; psql -c \"create schema pgml\" postgres://postgres@127.0.0.1:5433/pgml_development; ON_ERROR_STOP=1 psql -f sql/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development; cd .. Make sure to run it exactly like this, from the root directory of the repo.","title":"Maintain your development database"},{"location":"development/overview/#update-documentation","text":"mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Update documentation"},{"location":"guides/dashboard/","text":"Dashboard \u00b6 PostgresML comes with an app to provide visibility into models and datasets in your database. If you're running the standard docker container, you can view it running on http://localhost:8000/ . Since your pgml schema starts empty, there isn't much to see. If you'd like to generate some examples, you can run the test suite against your database. Generate example data \u00b6 The test suite for PostgresML is composed by running the sql files in the examples directory . You can use these examples to populate your local installation with some seed data. The test suite only operates on the pgml schema, and is otherwise isolated from the rest of the Postgres cluster. $ psql -f pgml-extension/sql/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development Overview \u00b6 Now there should be something to see in your local dashboard. Projects \u00b6 Projects organize Models that are all striving toward the same objective. They aren't much more than a name to group a collection of models. You can see the currently deployed model for each project indicated by . Models \u00b6 Models are the result of training an algorithm on a Snapshot of a dataset. They record metrics depending on their projects objective, and are scored accordingly. Some models are the result of a hyperparameter search, and include additional analysis on the range of hyperparameters they are tested against. Snapshots \u00b6 A Snapshot is created during training runs to record the data used for further analysis, or to train additional models against identical data. Deployments \u00b6 Every deployement is recorded to track models over time.","title":"Dashboard"},{"location":"guides/dashboard/#dashboard","text":"PostgresML comes with an app to provide visibility into models and datasets in your database. If you're running the standard docker container, you can view it running on http://localhost:8000/ . Since your pgml schema starts empty, there isn't much to see. If you'd like to generate some examples, you can run the test suite against your database.","title":"Dashboard"},{"location":"guides/dashboard/#generate-example-data","text":"The test suite for PostgresML is composed by running the sql files in the examples directory . You can use these examples to populate your local installation with some seed data. The test suite only operates on the pgml schema, and is otherwise isolated from the rest of the Postgres cluster. $ psql -f pgml-extension/sql/test.sql -P pager postgres://postgres@127.0.0.1:5433/pgml_development","title":"Generate example data"},{"location":"guides/dashboard/#overview","text":"Now there should be something to see in your local dashboard.","title":"Overview"},{"location":"guides/dashboard/#projects","text":"Projects organize Models that are all striving toward the same objective. They aren't much more than a name to group a collection of models. You can see the currently deployed model for each project indicated by .","title":"Projects"},{"location":"guides/dashboard/#models","text":"Models are the result of training an algorithm on a Snapshot of a dataset. They record metrics depending on their projects objective, and are scored accordingly. Some models are the result of a hyperparameter search, and include additional analysis on the range of hyperparameters they are tested against.","title":"Models"},{"location":"guides/dashboard/#snapshots","text":"A Snapshot is created during training runs to record the data used for further analysis, or to train additional models against identical data.","title":"Snapshots"},{"location":"guides/dashboard/#deployments","text":"Every deployement is recorded to track models over time.","title":"Deployments"},{"location":"guides/deployments/","text":"Deployments \u00b6 Models are automatically deployed if their key metric ( mean_squared_error for regression, f1 for classification) is improved over the currently deployed version during training. If you want to manage deploys manually, you can always change which model is currently responsible for making predictions. API \u00b6 pgml.deploy 1 2 3 4 5 pgml . deploy ( project_name TEXT , -- Human-friendly project name strategy TEXT DEFAULT 'best_score' , -- 'rollback', 'best_score', or 'most_recent' algorithm_name TEXT DEFAULT NULL -- filter candidates to a particular algorithm, NULL = all qualify ) The default behavior allows any algorithm to qualify, but deployment candidates can be further restricted to a specific algorithm by passing the algorithm_name . Note Deployed models are cached at the session level to improve prediction times. Active sessions will not see deploys until they reconnect. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'best_score' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row ) Strategies \u00b6 There are 3 different deployment strategies available strategy description most_recent The most recently trained model for this project best_score The model that achieved the best key metric score rollback The model that was previously deployed for this project Rolling back to a specific algorithm \u00b6 Rolling back creates a new deploy for the model that was deployed before the current one. Multiple rollbacks in a row will effectively oscilate between the two most recently deployed models, making rollbacks a relatively safe operation. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'rollback' , 'svm' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row )","title":"Deployments"},{"location":"guides/deployments/#deployments","text":"Models are automatically deployed if their key metric ( mean_squared_error for regression, f1 for classification) is improved over the currently deployed version during training. If you want to manage deploys manually, you can always change which model is currently responsible for making predictions.","title":"Deployments"},{"location":"guides/deployments/#api","text":"pgml.deploy 1 2 3 4 5 pgml . deploy ( project_name TEXT , -- Human-friendly project name strategy TEXT DEFAULT 'best_score' , -- 'rollback', 'best_score', or 'most_recent' algorithm_name TEXT DEFAULT NULL -- filter candidates to a particular algorithm, NULL = all qualify ) The default behavior allows any algorithm to qualify, but deployment candidates can be further restricted to a specific algorithm by passing the algorithm_name . Note Deployed models are cached at the session level to improve prediction times. Active sessions will not see deploys until they reconnect. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'best_score' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row )","title":"API"},{"location":"guides/deployments/#strategies","text":"There are 3 different deployment strategies available strategy description most_recent The most recently trained model for this project best_score The model that achieved the best key metric score rollback The model that was previously deployed for this project","title":"Strategies"},{"location":"guides/deployments/#rolling-back-to-a-specific-algorithm","text":"Rolling back creates a new deploy for the model that was deployed before the current one. Multiple rollbacks in a row will effectively oscilate between the two most recently deployed models, making rollbacks a relatively safe operation. SQL Output 1 SELECT * FROM pgml . deploy ( 'Handwritten Digit Image Classifier' , 'rollback' , 'svm' ); 1 2 3 4 project_name | strategy | algorithm_name ------------------------------------+----------------+---------------- Handwritten Digit Image Classifier | classification | linear ( 1 row )","title":"Rolling back to a specific algorithm"},{"location":"guides/hyperparameters/","text":"Hyperparameter tuning \u00b6 Models can be further refined with the scikit cross validation hyperparameter search libraries. We currently support the grid and random implementations. API \u00b6 The arguments to pgml.train that begin with search are used for hyperparameter turning. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name objective TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) search can either be grid or random . search_params is the set of hyperparameters to search for your algorithm search_args are passed to the scikit learn model selection algorithm for extra configuration search description grid Trains every permutation of search_params random Randomly samples search_params to train models Hyperparameter Search \u00b6 This grid search will train len(max_depth) * len(n_estimators) * len(learning_rate) = 6 * 4 * 4 = 96 combinations to compare all possible permutations of the search_params . It takes a couple of minutes on my computer, but you can delete some values if you want to speed things up. I like to watch all cores operate at 100% utilization in a seperate terminal with htop. SQL Output SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , algorithm => 'xgboost' , search => 'grid' , search_params => '{ \"max_depth\": [1, 2, 3, 4, 5, 6], \"n_estimators\": [20, 40, 80, 160], \"learning_rate\": [0.1, 0.2, 0.3, 0.4] }' ); project_name | objective | algorithm_name | status ------------------------------------+-----------+----------------+---------- Handwritten Digit Image Classifier | | xgboost | deployed (1 row) As you can see from the output, a new set model has been deployed with a better performance. There will also be a new analysis available on this model visible in the dashboard. Visualization \u00b6 The optimal set of hyperparams is chosen for the model, and that combination is highlighted among all search candidates. The impact of each hyperparameter is measured against the key metric, as well as the training and test times. In this particular case, it's interesting that as max_depth increases, the \"Test Score\" on the key metric trends lower, so the smallest value of max_depth is chosen to maximize the \"Test Score\". Luckily, the smallest max_depth values also have the fastest \"Fit Time\", indicating that we pay less for training these higher quality models. It's a little less obvious how the different values n_estimators and learning_rate impact the test score. We may want to rerun our search and zoom in our out in the search space to get more insight.","title":"Hyperparameters"},{"location":"guides/hyperparameters/#hyperparameter-tuning","text":"Models can be further refined with the scikit cross validation hyperparameter search libraries. We currently support the grid and random implementations.","title":"Hyperparameter tuning"},{"location":"guides/hyperparameters/#api","text":"The arguments to pgml.train that begin with search are used for hyperparameter turning. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name objective TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) search can either be grid or random . search_params is the set of hyperparameters to search for your algorithm search_args are passed to the scikit learn model selection algorithm for extra configuration search description grid Trains every permutation of search_params random Randomly samples search_params to train models","title":"API"},{"location":"guides/hyperparameters/#hyperparameter-search","text":"This grid search will train len(max_depth) * len(n_estimators) * len(learning_rate) = 6 * 4 * 4 = 96 combinations to compare all possible permutations of the search_params . It takes a couple of minutes on my computer, but you can delete some values if you want to speed things up. I like to watch all cores operate at 100% utilization in a seperate terminal with htop. SQL Output SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , algorithm => 'xgboost' , search => 'grid' , search_params => '{ \"max_depth\": [1, 2, 3, 4, 5, 6], \"n_estimators\": [20, 40, 80, 160], \"learning_rate\": [0.1, 0.2, 0.3, 0.4] }' ); project_name | objective | algorithm_name | status ------------------------------------+-----------+----------------+---------- Handwritten Digit Image Classifier | | xgboost | deployed (1 row) As you can see from the output, a new set model has been deployed with a better performance. There will also be a new analysis available on this model visible in the dashboard.","title":"Hyperparameter Search"},{"location":"guides/hyperparameters/#visualization","text":"The optimal set of hyperparams is chosen for the model, and that combination is highlighted among all search candidates. The impact of each hyperparameter is measured against the key metric, as well as the training and test times. In this particular case, it's interesting that as max_depth increases, the \"Test Score\" on the key metric trends lower, so the smallest value of max_depth is chosen to maximize the \"Test Score\". Luckily, the smallest max_depth values also have the fastest \"Fit Time\", indicating that we pay less for training these higher quality models. It's a little less obvious how the different values n_estimators and learning_rate impact the test score. We may want to rerun our search and zoom in our out in the search space to get more insight.","title":"Visualization"},{"location":"guides/installation/","text":"Installation \u00b6 with Docker Recommended \u00b6 The quickest way to get started is with Docker: Docker for Linux . Some package managers (e.g. Ubuntu/Debian) additionally require the docker-compose package to be installed seperately. Docker for OS X Docker for Windows . Use the Linux instructions if you're installing in Windows Subsystem for Linux. Starting up a local system is then as simple as: $ docker-compose up -d PostgresML will run on port 5433, just in case you already have Postgres running. Then to connect, run: $ psql postgres://postgres@localhost:5433/pgml_development To validate it works, you can execute this query and you should see this result: pgml_development =# SELECT pgml . version (); version --------- 0 . 8 . 1 ( 1 row ) Docker Compose will also start the admin app running locally on port 8000 http://localhost:8000/ Native Installation & Production Deployments \u00b6 A PostgresML deployment consists of two different runtimes. The foundational runtime is a Python extension for Postgres ( pgml-extension ) that facilitates the machine learning lifecycle inside the database. Additionally, we provide a dashboard ( pgml-admin ) that can connect to your Postgres server and provide additional management functionality. It will also provide visibility into the models you build and data they use. Mac OS \u00b6 We recommend you use Postgres.app because it comes with PL/Python , the extension we rely on, built into the installation. Otherwise, you'll need to install PL/Python. Once you have Postgres.app running, you'll need to install the Python framework. Mac OS has multiple distributions of Python, namely one from Brew and one from the Python community (Python.org); Postgres.app and PL/Python depend on the community one. The following versions of Python and Postgres.app are compatible: PostgreSQL version Python version Download link 14 3.9 Python 3.9 64-bit 13 3.8 Python 3.8 64-bit All Python.org installers for Mac OS are available here . You can also get more details about this in the Postgres.app documentation . Python package \u00b6 To use our Python package inside Postgres, we need to install it into the global Python package space. Depending on which version of Python you installed in the previous step, use the correspoding pip executable. Since Python was installed as a framework, sudo (root) is not required. For PostgreSQL 14, use Python & Pip 3.9: $ pip3.9 install pgml-extension PL/Python functions \u00b6 Finally to interact with the package, install our functions and supporting tables into the database: $ psql -f sql/install.sql If everything works, you should be able to run this successfully: $ psql -c 'SELECT pgml.version()' Ubuntu/Debian \u00b6 Each Ubuntu/Debian distribution comes with its own version of PostgreSQL, the simplest way is to install it from Aptitude: $ sudo apt-get install -y postgresql-plpython3-12 python3 python3-pip postgresql-12 Restart PostgreSQL: $ sudo service postgresql restart Install our Python package and SQL functions: $ sudo pip3 install pgml-extension $ psql -f sql/install.sql If everything works correctly, you should be able to run this successfully: $ psql -c 'SELECT pgml.version()'","title":"Installation"},{"location":"guides/installation/#installation","text":"","title":"Installation"},{"location":"guides/installation/#with-docker-recommended","text":"The quickest way to get started is with Docker: Docker for Linux . Some package managers (e.g. Ubuntu/Debian) additionally require the docker-compose package to be installed seperately. Docker for OS X Docker for Windows . Use the Linux instructions if you're installing in Windows Subsystem for Linux. Starting up a local system is then as simple as: $ docker-compose up -d PostgresML will run on port 5433, just in case you already have Postgres running. Then to connect, run: $ psql postgres://postgres@localhost:5433/pgml_development To validate it works, you can execute this query and you should see this result: pgml_development =# SELECT pgml . version (); version --------- 0 . 8 . 1 ( 1 row ) Docker Compose will also start the admin app running locally on port 8000 http://localhost:8000/","title":"with Docker Recommended"},{"location":"guides/installation/#native-installation-production-deployments","text":"A PostgresML deployment consists of two different runtimes. The foundational runtime is a Python extension for Postgres ( pgml-extension ) that facilitates the machine learning lifecycle inside the database. Additionally, we provide a dashboard ( pgml-admin ) that can connect to your Postgres server and provide additional management functionality. It will also provide visibility into the models you build and data they use.","title":"Native Installation &amp; Production Deployments"},{"location":"guides/installation/#mac-os","text":"We recommend you use Postgres.app because it comes with PL/Python , the extension we rely on, built into the installation. Otherwise, you'll need to install PL/Python. Once you have Postgres.app running, you'll need to install the Python framework. Mac OS has multiple distributions of Python, namely one from Brew and one from the Python community (Python.org); Postgres.app and PL/Python depend on the community one. The following versions of Python and Postgres.app are compatible: PostgreSQL version Python version Download link 14 3.9 Python 3.9 64-bit 13 3.8 Python 3.8 64-bit All Python.org installers for Mac OS are available here . You can also get more details about this in the Postgres.app documentation .","title":"Mac OS"},{"location":"guides/installation/#python-package","text":"To use our Python package inside Postgres, we need to install it into the global Python package space. Depending on which version of Python you installed in the previous step, use the correspoding pip executable. Since Python was installed as a framework, sudo (root) is not required. For PostgreSQL 14, use Python & Pip 3.9: $ pip3.9 install pgml-extension","title":"Python package"},{"location":"guides/installation/#plpython-functions","text":"Finally to interact with the package, install our functions and supporting tables into the database: $ psql -f sql/install.sql If everything works, you should be able to run this successfully: $ psql -c 'SELECT pgml.version()'","title":"PL/Python functions"},{"location":"guides/installation/#ubuntudebian","text":"Each Ubuntu/Debian distribution comes with its own version of PostgreSQL, the simplest way is to install it from Aptitude: $ sudo apt-get install -y postgresql-plpython3-12 python3 python3-pip postgresql-12 Restart PostgreSQL: $ sudo service postgresql restart Install our Python package and SQL functions: $ sudo pip3 install pgml-extension $ psql -f sql/install.sql If everything works correctly, you should be able to run this successfully: $ psql -c 'SELECT pgml.version()'","title":"Ubuntu/Debian"},{"location":"guides/predictions/","text":"Predictions \u00b6 The predict function is the key value proposition of PostgresML. It provides online predictions using the actively deployed model for a project. API \u00b6 pgml.predict 1 2 3 4 pgml . predict ( project_name TEXT , -- Human-friendly project name features DOUBLE PRECISION [] -- Must match the training data column order ) Example Once a model has been trained for a project, making predictions is as simple as: 1 2 3 4 SELECT pgml . predict ( 'Human-friendly project name' , ARRAY [...] ) AS prediction_score ; where ARRAY[...] is the same list of features for a sample used in training. This score can be used in normal queries, for example: 1 2 3 4 5 6 7 8 9 SELECT * , pgml . predict ( 'Probability of buying our products' , ARRAY [ user . location , NOW () - user . created_at , user . total_purchases_in_dollars ] ) AS likely_to_buy_score FROM users WHERE comapany_id = 5 ORDER BY likely_to_buy_score LIMIT 25 ; Making Predictions \u00b6 If you've already done the training guide , you can see the results of those efforts: SQL Output 1 2 3 SELECT target , pgml . predict ( 'Handwritten Digit Image Classifier' , image ) AS prediction FROM pgml . digits LIMIT 10 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 ( 10 rows ) Checking the deployed algorithm \u00b6 If you're ever curious about which deployed models will be used to make predictions, you can see them in the pgml.deployed_models VIEW. SQL Output 1 SELECT * FROM pgml . deployed_models ; 1 2 3 id | name | objective | algorithm_name | deployed_at ----+------------------------------------+----------------+----------------+---------------------------- 1 | Handwritten Digit Image Classifier | classification | linear | 2022 - 05 - 10 15 : 28 : 53 . 383893","title":"Predictions"},{"location":"guides/predictions/#predictions","text":"The predict function is the key value proposition of PostgresML. It provides online predictions using the actively deployed model for a project.","title":"Predictions"},{"location":"guides/predictions/#api","text":"pgml.predict 1 2 3 4 pgml . predict ( project_name TEXT , -- Human-friendly project name features DOUBLE PRECISION [] -- Must match the training data column order ) Example Once a model has been trained for a project, making predictions is as simple as: 1 2 3 4 SELECT pgml . predict ( 'Human-friendly project name' , ARRAY [...] ) AS prediction_score ; where ARRAY[...] is the same list of features for a sample used in training. This score can be used in normal queries, for example: 1 2 3 4 5 6 7 8 9 SELECT * , pgml . predict ( 'Probability of buying our products' , ARRAY [ user . location , NOW () - user . created_at , user . total_purchases_in_dollars ] ) AS likely_to_buy_score FROM users WHERE comapany_id = 5 ORDER BY likely_to_buy_score LIMIT 25 ;","title":"API"},{"location":"guides/predictions/#making-predictions","text":"If you've already done the training guide , you can see the results of those efforts: SQL Output 1 2 3 SELECT target , pgml . predict ( 'Handwritten Digit Image Classifier' , image ) AS prediction FROM pgml . digits LIMIT 10 ; 1 2 3 4 5 6 7 8 9 10 11 12 13 target | prediction --------+------------ 0 | 0 1 | 1 2 | 2 3 | 3 4 | 4 5 | 5 6 | 6 7 | 7 8 | 8 9 | 9 ( 10 rows )","title":"Making Predictions"},{"location":"guides/predictions/#checking-the-deployed-algorithm","text":"If you're ever curious about which deployed models will be used to make predictions, you can see them in the pgml.deployed_models VIEW. SQL Output 1 SELECT * FROM pgml . deployed_models ; 1 2 3 id | name | objective | algorithm_name | deployed_at ----+------------------------------------+----------------+----------------+---------------------------- 1 | Handwritten Digit Image Classifier | classification | linear | 2022 - 05 - 10 15 : 28 : 53 . 383893","title":"Checking the deployed algorithm"},{"location":"guides/training/","text":"Training \u00b6 The training function is at the heart of PostgresML. It's a powerful single call that can handle the different objectives of training depending on the arguments passed. API \u00b6 Most parameters are optional other than the project_name which is a simple human readable identifier to organize your work. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name objective TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) Example A minimal first call for a project looks like: SELECT * FROM pgml . train ( 'My Classification Project' , 'classification' , 'my_table_name' , 'my_tables_target_column_name' ); The train function requires an objective the first time a project_name is used. That objective is either regression or classification , which determines the relevant metrics and analysis performed for models trained toward a common goal. It also requires a relation_name and y_column_name that will be used to establish the first Snapshot of training and test data. By default, 25% of the data (specified by test_size ) will be randomly sampled to measure the performance of the model after the algorithm has been fit to the rest. Tip Postgres supports named arguments for function calls, which allows you to pass only the arguments you need. pgml . train ( 'Project Name' , algorithm => 'xgboost' ) Future calls to train may restate the same objective for a project, or omit it, but can't change it. Projects manage their active model using the metrics relevant to a particular objective, so changing it would mean some models in the project are no longer directly comparable. In that case, it's better to start a new project. Note If you'd like to train multiple models on the same Snapshot , follow up calls to train may omit the relation_name , y_column_name , test_size and test_sampling arguments to reuse identical data with multiple algorithms or hyperparams. The Snapshot is also saved after training runs for any follow up analysis required. See Algorithms for a complete list of supported options and their hyperparams. Getting training data \u00b6 A large part of machine learning is acquiring, cleaning and preparing data for algorithms. Naturally, we think Postgres is a great place to store your data. For the purpose of this example, we'll load a toy dataset, a classic handwritten digits image collection from scikit-learn. SQL Output 1 pgml_development =# SELECT pgml . load_dataset ( 'digits' ); 1 2 3 4 5 NOTICE : table \"digits\" does not exist , skipping -- (1) load_dataset -------------- OK ( 1 row ) This NOTICE can safely be ignored. PostgresML attempts to do a clean reload by dropping the pgml.digits table if it exists. The first time this command is run, the table does not exist. PostgresML loads this into a fixed table pgml.digits . You can examine the 2D arrays of image data, as well as the label in the target column. SQL Output 1 pgml_development =# SELECT target , image FROM pgml . digits LIMIT 5 ; 1 2 3 4 5 6 7 8 target | image --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 | {{ 0 , 0 , 5 , 13 , 9 , 1 , 0 , 0 } , { 0 , 0 , 13 , 15 , 10 , 15 , 5 , 0 } , { 0 , 3 , 15 , 2 , 0 , 11 , 8 , 0 } , { 0 , 4 , 12 , 0 , 0 , 8 , 8 , 0 } , { 0 , 5 , 8 , 0 , 0 , 9 , 8 , 0 } , { 0 , 4 , 11 , 0 , 1 , 12 , 7 , 0 } , { 0 , 2 , 14 , 5 , 10 , 12 , 0 , 0 } , { 0 , 0 , 6 , 13 , 10 , 0 , 0 , 0 }} 1 | {{ 0 , 0 , 0 , 12 , 13 , 5 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 9 , 0 , 0 } , { 0 , 0 , 3 , 15 , 16 , 6 , 0 , 0 } , { 0 , 7 , 15 , 16 , 16 , 2 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 3 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 10 , 0 , 0 }} 2 | {{ 0 , 0 , 0 , 4 , 15 , 12 , 0 , 0 } , { 0 , 0 , 3 , 16 , 15 , 14 , 0 , 0 } , { 0 , 0 , 8 , 13 , 8 , 16 , 0 , 0 } , { 0 , 0 , 1 , 6 , 15 , 11 , 0 , 0 } , { 0 , 1 , 8 , 13 , 15 , 1 , 0 , 0 } , { 0 , 9 , 16 , 16 , 5 , 0 , 0 , 0 } , { 0 , 3 , 13 , 16 , 16 , 11 , 5 , 0 } , { 0 , 0 , 0 , 3 , 11 , 16 , 9 , 0 }} 3 | {{ 0 , 0 , 7 , 15 , 13 , 1 , 0 , 0 } , { 0 , 8 , 13 , 6 , 15 , 4 , 0 , 0 } , { 0 , 2 , 1 , 13 , 13 , 0 , 0 , 0 } , { 0 , 0 , 2 , 15 , 11 , 1 , 0 , 0 } , { 0 , 0 , 0 , 1 , 12 , 12 , 1 , 0 } , { 0 , 0 , 0 , 0 , 1 , 10 , 8 , 0 } , { 0 , 0 , 8 , 4 , 5 , 14 , 9 , 0 } , { 0 , 0 , 7 , 13 , 13 , 9 , 0 , 0 }} 4 | {{ 0 , 0 , 0 , 1 , 11 , 0 , 0 , 0 } , { 0 , 0 , 0 , 7 , 8 , 0 , 0 , 0 } , { 0 , 0 , 1 , 13 , 6 , 2 , 2 , 0 } , { 0 , 0 , 7 , 15 , 0 , 9 , 8 , 0 } , { 0 , 5 , 16 , 10 , 0 , 16 , 6 , 0 } , { 0 , 4 , 15 , 16 , 13 , 16 , 1 , 0 } , { 0 , 0 , 0 , 3 , 15 , 10 , 0 , 0 } , { 0 , 0 , 0 , 2 , 16 , 4 , 0 , 0 }} ( 5 rows ) Training the model \u00b6 Now that we've got data, we're ready to train a model using an algorithm. We'll start with the default linear algorithm to demonstrate the basics. See the Algorithms reference for a complete list of available choices. SQL Output 1 SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , 'classification' , 'pgml.digits' , 'target' ); sql linenumes=\"1\" project_name | objective | algorithm_name | status ------------------------------------+----------------+----------------+---------- Handwritten Digit Image Classifier | classification | linear | deployed (1 row) The output gives us a pieces of information about the training run, including the deployed status. This is great news indicating training has successfully reached a new high score for the project's key metric and our new model was automatically deployed as the one that will be used to make new predictions for the project. See Deployments for a guide to managing the active model. Inspecting the results \u00b6 Now we can inspect some of the artifacts a training run creates. SQL Output 1 SELECT * FROM pgml . overview ; 1 2 3 4 name | deployed_at | objective | algorithm_name | relation_name | y_column_name | test_sampling | test_size ------------------------------------+----------------------------+----------------+----------------+---------------+---------------+---------------+----------- Handwritten Digit Image Classifier | 2022 - 05 - 10 15 : 06 : 32 . 824305 | classification | linear | pgml . digits | { target } | random | 0 . 25 ( 1 row ) See the Schema reference for a complete description of all artifacts.","title":"Training"},{"location":"guides/training/#training","text":"The training function is at the heart of PostgresML. It's a powerful single call that can handle the different objectives of training depending on the arguments passed.","title":"Training"},{"location":"guides/training/#api","text":"Most parameters are optional other than the project_name which is a simple human readable identifier to organize your work. pgml.train 1 2 3 4 5 6 7 8 9 10 11 12 13 pgml . train ( project_name TEXT , -- Human-friendly project name objective TEXT DEFAULT NULL , -- 'regression' or 'classification' relation_name TEXT DEFAULT NULL , -- name of table or view y_column_name TEXT DEFAULT NULL , -- aka \"label\" or \"unknown\" or \"target\" algorithm TEXT DEFAULT 'linear' , -- statistical learning method hyperparams JSONB DEFAULT '{}' :: JSONB , -- options for the model search TEXT DEFAULT NULL , -- hyperparam tuning, 'grid' or 'random' search_params JSONB DEFAULT '{}' :: JSONB , -- hyperparam search space search_args JSONB DEFAULT '{}' :: JSONB , -- hyperparam options test_size REAL DEFAULT 0 . 25 , -- fraction of the data for the test set test_sampling TEXT DEFAULT 'random' -- 'random', 'first' or 'last' ) Example A minimal first call for a project looks like: SELECT * FROM pgml . train ( 'My Classification Project' , 'classification' , 'my_table_name' , 'my_tables_target_column_name' ); The train function requires an objective the first time a project_name is used. That objective is either regression or classification , which determines the relevant metrics and analysis performed for models trained toward a common goal. It also requires a relation_name and y_column_name that will be used to establish the first Snapshot of training and test data. By default, 25% of the data (specified by test_size ) will be randomly sampled to measure the performance of the model after the algorithm has been fit to the rest. Tip Postgres supports named arguments for function calls, which allows you to pass only the arguments you need. pgml . train ( 'Project Name' , algorithm => 'xgboost' ) Future calls to train may restate the same objective for a project, or omit it, but can't change it. Projects manage their active model using the metrics relevant to a particular objective, so changing it would mean some models in the project are no longer directly comparable. In that case, it's better to start a new project. Note If you'd like to train multiple models on the same Snapshot , follow up calls to train may omit the relation_name , y_column_name , test_size and test_sampling arguments to reuse identical data with multiple algorithms or hyperparams. The Snapshot is also saved after training runs for any follow up analysis required. See Algorithms for a complete list of supported options and their hyperparams.","title":"API"},{"location":"guides/training/#getting-training-data","text":"A large part of machine learning is acquiring, cleaning and preparing data for algorithms. Naturally, we think Postgres is a great place to store your data. For the purpose of this example, we'll load a toy dataset, a classic handwritten digits image collection from scikit-learn. SQL Output 1 pgml_development =# SELECT pgml . load_dataset ( 'digits' ); 1 2 3 4 5 NOTICE : table \"digits\" does not exist , skipping -- (1) load_dataset -------------- OK ( 1 row ) This NOTICE can safely be ignored. PostgresML attempts to do a clean reload by dropping the pgml.digits table if it exists. The first time this command is run, the table does not exist. PostgresML loads this into a fixed table pgml.digits . You can examine the 2D arrays of image data, as well as the label in the target column. SQL Output 1 pgml_development =# SELECT target , image FROM pgml . digits LIMIT 5 ; 1 2 3 4 5 6 7 8 target | image --------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------- 0 | {{ 0 , 0 , 5 , 13 , 9 , 1 , 0 , 0 } , { 0 , 0 , 13 , 15 , 10 , 15 , 5 , 0 } , { 0 , 3 , 15 , 2 , 0 , 11 , 8 , 0 } , { 0 , 4 , 12 , 0 , 0 , 8 , 8 , 0 } , { 0 , 5 , 8 , 0 , 0 , 9 , 8 , 0 } , { 0 , 4 , 11 , 0 , 1 , 12 , 7 , 0 } , { 0 , 2 , 14 , 5 , 10 , 12 , 0 , 0 } , { 0 , 0 , 6 , 13 , 10 , 0 , 0 , 0 }} 1 | {{ 0 , 0 , 0 , 12 , 13 , 5 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 9 , 0 , 0 } , { 0 , 0 , 3 , 15 , 16 , 6 , 0 , 0 } , { 0 , 7 , 15 , 16 , 16 , 2 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 3 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 1 , 16 , 16 , 6 , 0 , 0 } , { 0 , 0 , 0 , 11 , 16 , 10 , 0 , 0 }} 2 | {{ 0 , 0 , 0 , 4 , 15 , 12 , 0 , 0 } , { 0 , 0 , 3 , 16 , 15 , 14 , 0 , 0 } , { 0 , 0 , 8 , 13 , 8 , 16 , 0 , 0 } , { 0 , 0 , 1 , 6 , 15 , 11 , 0 , 0 } , { 0 , 1 , 8 , 13 , 15 , 1 , 0 , 0 } , { 0 , 9 , 16 , 16 , 5 , 0 , 0 , 0 } , { 0 , 3 , 13 , 16 , 16 , 11 , 5 , 0 } , { 0 , 0 , 0 , 3 , 11 , 16 , 9 , 0 }} 3 | {{ 0 , 0 , 7 , 15 , 13 , 1 , 0 , 0 } , { 0 , 8 , 13 , 6 , 15 , 4 , 0 , 0 } , { 0 , 2 , 1 , 13 , 13 , 0 , 0 , 0 } , { 0 , 0 , 2 , 15 , 11 , 1 , 0 , 0 } , { 0 , 0 , 0 , 1 , 12 , 12 , 1 , 0 } , { 0 , 0 , 0 , 0 , 1 , 10 , 8 , 0 } , { 0 , 0 , 8 , 4 , 5 , 14 , 9 , 0 } , { 0 , 0 , 7 , 13 , 13 , 9 , 0 , 0 }} 4 | {{ 0 , 0 , 0 , 1 , 11 , 0 , 0 , 0 } , { 0 , 0 , 0 , 7 , 8 , 0 , 0 , 0 } , { 0 , 0 , 1 , 13 , 6 , 2 , 2 , 0 } , { 0 , 0 , 7 , 15 , 0 , 9 , 8 , 0 } , { 0 , 5 , 16 , 10 , 0 , 16 , 6 , 0 } , { 0 , 4 , 15 , 16 , 13 , 16 , 1 , 0 } , { 0 , 0 , 0 , 3 , 15 , 10 , 0 , 0 } , { 0 , 0 , 0 , 2 , 16 , 4 , 0 , 0 }} ( 5 rows )","title":"Getting training data"},{"location":"guides/training/#training-the-model","text":"Now that we've got data, we're ready to train a model using an algorithm. We'll start with the default linear algorithm to demonstrate the basics. See the Algorithms reference for a complete list of available choices. SQL Output 1 SELECT * FROM pgml . train ( 'Handwritten Digit Image Classifier' , 'classification' , 'pgml.digits' , 'target' ); sql linenumes=\"1\" project_name | objective | algorithm_name | status ------------------------------------+----------------+----------------+---------- Handwritten Digit Image Classifier | classification | linear | deployed (1 row) The output gives us a pieces of information about the training run, including the deployed status. This is great news indicating training has successfully reached a new high score for the project's key metric and our new model was automatically deployed as the one that will be used to make new predictions for the project. See Deployments for a guide to managing the active model.","title":"Training the model"},{"location":"guides/training/#inspecting-the-results","text":"Now we can inspect some of the artifacts a training run creates. SQL Output 1 SELECT * FROM pgml . overview ; 1 2 3 4 name | deployed_at | objective | algorithm_name | relation_name | y_column_name | test_sampling | test_size ------------------------------------+----------------------------+----------------+----------------+---------------+---------------+---------------+----------- Handwritten Digit Image Classifier | 2022 - 05 - 10 15 : 06 : 32 . 824305 | classification | linear | pgml . digits | { target } | random | 0 . 25 ( 1 row ) See the Schema reference for a complete description of all artifacts.","title":"Inspecting the results"},{"location":"guides/vectors/","text":"Vector Operations \u00b6 PostgresML adds native vector operations that can be called from SQL: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- Elementwise arithmetic w/ constants pgml . add ( a REAL [], b REAL ) -> REAL [] pgml . subtract ( minuend REAL [], subtrahend REAL ) -> REAL [] pgml . multiply ( multiplicand REAL [], multiplier REAL ) -> REAL [] pgml . divide ( dividend REAL [], divisor REAL ) -> REAL [] -- Pairwise arithmetic w/ vectors pgml . add ( a REAL [], b REAL []) -> REAL [] pgml . subtract ( minuend REAL [], subtrahend REAL []) -> REAL [] pgml . multiply ( multiplicand REAL [], multiplier REAL []) -> REAL [] pgml . divide ( dividend REAL [], divisor REAL []) -> REAL [] -- Norms pgml . norm_l0 ( vector REAL []) -> REAL -- Dimensions not at the origin pgml . norm_l1 ( vector REAL []) -> REAL -- Manhattan distance from origin pgml . norm_l2 ( vector REAL []) -> REAL -- Euclidean distance from origin pgml . norm_max ( vector REAL []) -> REAL -- Absolute value of largest element -- Normalization pgml . normalize_l1 ( vector REAL []) -> REAL [] -- Unit Vector pgml . normalize_l2 ( vector REAL []) -> REAL [] -- Squared Unit Vector pgml . normalize_max ( vector REAL []) -> REAL [] -- -1:1 values -- Distances pgml . distance_l1 ( a REAL [], b REAL []) -> REAL -- Manhattan pgml . distance_l2 ( a REAL [], b REAL []) -> REAL -- Euclidean pgml . dot_product ( a REAL [], b REAL []) -> REAL -- Projection pgml . cosine_similarity ( a REAL [], b REAL []) -> REAL -- Direction","title":"Vectors"},{"location":"guides/vectors/#vector-operations","text":"PostgresML adds native vector operations that can be called from SQL: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 -- Elementwise arithmetic w/ constants pgml . add ( a REAL [], b REAL ) -> REAL [] pgml . subtract ( minuend REAL [], subtrahend REAL ) -> REAL [] pgml . multiply ( multiplicand REAL [], multiplier REAL ) -> REAL [] pgml . divide ( dividend REAL [], divisor REAL ) -> REAL [] -- Pairwise arithmetic w/ vectors pgml . add ( a REAL [], b REAL []) -> REAL [] pgml . subtract ( minuend REAL [], subtrahend REAL []) -> REAL [] pgml . multiply ( multiplicand REAL [], multiplier REAL []) -> REAL [] pgml . divide ( dividend REAL [], divisor REAL []) -> REAL [] -- Norms pgml . norm_l0 ( vector REAL []) -> REAL -- Dimensions not at the origin pgml . norm_l1 ( vector REAL []) -> REAL -- Manhattan distance from origin pgml . norm_l2 ( vector REAL []) -> REAL -- Euclidean distance from origin pgml . norm_max ( vector REAL []) -> REAL -- Absolute value of largest element -- Normalization pgml . normalize_l1 ( vector REAL []) -> REAL [] -- Unit Vector pgml . normalize_l2 ( vector REAL []) -> REAL [] -- Squared Unit Vector pgml . normalize_max ( vector REAL []) -> REAL [] -- -1:1 values -- Distances pgml . distance_l1 ( a REAL [], b REAL []) -> REAL -- Manhattan pgml . distance_l2 ( a REAL [], b REAL []) -> REAL -- Euclidean pgml . dot_product ( a REAL [], b REAL []) -> REAL -- Projection pgml . cosine_similarity ( a REAL [], b REAL []) -> REAL -- Direction","title":"Vector Operations"},{"location":"references/algorithms/","text":"Algorithms \u00b6 We currently support the following regression and classification models from Scikit-Learn and XGBoost . XGBoost \u00b6 Algorithm Regression Classification xgboost XGBRegressor XGBClassifier xgboost_random_forest XGBRFRegressor XGBRFClassifier Scikit Ensembles \u00b6 Algorithm Regression Classification ada_boost AdaBoostRegressor AdaBoostClassifier bagging BaggingRegressor BaggingClassifier extra_trees ExtraTreesRegressor ExtraTreesClassifier gradient_boosting_trees GradientBoostingRegressor GradientBoostingClassifier random_forest RandomForestRegressor RandomForestClassifier hist_gradient_boosting HistGradientBoostingRegressor HistGradientBoostingClassifier Support Vector Machines \u00b6 Algorithm Regression Classification svm SVR SVC nu_svm NuSVR NuSVC linear_svm LinearSVR LinearSVC Linear Models \u00b6 Algorithm Regression Classification linear LinearRegression LogisticRegression ridge Ridge RidgeClassifier lasso Lasso - elastic_net ElasticNet - least_angle LARS - lasso_least_angle LassoLars - orthoganl_matching_pursuit OrthogonalMatchingPursuit - bayesian_ridge BayesianRidge - automatic_relevance_determination ARDRegression - stochastic_gradient_descent SGDRegressor SGDClassifier perceptron - Perceptron passive_aggressive PassiveAggressiveRegressor PassiveAggressiveClassifier ransac RANSACRegressor - theil_sen TheilSenRegressor - huber HuberRegressor - quantile QuantileRegressor - Other \u00b6 Algorithm Regression Classification kernel_ridge KernelRidge - gaussian_process GaussianProcessRegressor GaussianProcessClassifier","title":"Algorithms"},{"location":"references/algorithms/#algorithms","text":"We currently support the following regression and classification models from Scikit-Learn and XGBoost .","title":"Algorithms"},{"location":"references/algorithms/#xgboost","text":"Algorithm Regression Classification xgboost XGBRegressor XGBClassifier xgboost_random_forest XGBRFRegressor XGBRFClassifier","title":"XGBoost"},{"location":"references/algorithms/#scikit-ensembles","text":"Algorithm Regression Classification ada_boost AdaBoostRegressor AdaBoostClassifier bagging BaggingRegressor BaggingClassifier extra_trees ExtraTreesRegressor ExtraTreesClassifier gradient_boosting_trees GradientBoostingRegressor GradientBoostingClassifier random_forest RandomForestRegressor RandomForestClassifier hist_gradient_boosting HistGradientBoostingRegressor HistGradientBoostingClassifier","title":"Scikit Ensembles"},{"location":"references/algorithms/#support-vector-machines","text":"Algorithm Regression Classification svm SVR SVC nu_svm NuSVR NuSVC linear_svm LinearSVR LinearSVC","title":"Support Vector Machines"},{"location":"references/algorithms/#linear-models","text":"Algorithm Regression Classification linear LinearRegression LogisticRegression ridge Ridge RidgeClassifier lasso Lasso - elastic_net ElasticNet - least_angle LARS - lasso_least_angle LassoLars - orthoganl_matching_pursuit OrthogonalMatchingPursuit - bayesian_ridge BayesianRidge - automatic_relevance_determination ARDRegression - stochastic_gradient_descent SGDRegressor SGDClassifier perceptron - Perceptron passive_aggressive PassiveAggressiveRegressor PassiveAggressiveClassifier ransac RANSACRegressor - theil_sen TheilSenRegressor - huber HuberRegressor - quantile QuantileRegressor -","title":"Linear Models"},{"location":"references/algorithms/#other","text":"Algorithm Regression Classification kernel_ridge KernelRidge - gaussian_process GaussianProcessRegressor GaussianProcessClassifier","title":"Other"},{"location":"references/deployments/","text":"Deployments \u00b6 Deployments are an artifact of calls to pgml.deploy . See deployments for ways to create new deployments. Schema \u00b6 sql linenums=\"1\"c pgml.deployments( id BIGSERIAL PRIMARY KEY, project_id BIGINT NOT NULL, model_id BIGINT NOT NULL, strategy TEXT NOT NULL, created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp(), CONSTRAINT project_id_fk FOREIGN KEY(project_id) REFERENCES pgml.projects(id), CONSTRAINT model_id_fk FOREIGN KEY(model_id) REFERENCES pgml.models(id) );","title":"Deployments"},{"location":"references/deployments/#deployments","text":"Deployments are an artifact of calls to pgml.deploy . See deployments for ways to create new deployments.","title":"Deployments"},{"location":"references/deployments/#schema","text":"sql linenums=\"1\"c pgml.deployments( id BIGSERIAL PRIMARY KEY, project_id BIGINT NOT NULL, model_id BIGINT NOT NULL, strategy TEXT NOT NULL, created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp(), CONSTRAINT project_id_fk FOREIGN KEY(project_id) REFERENCES pgml.projects(id), CONSTRAINT model_id_fk FOREIGN KEY(model_id) REFERENCES pgml.models(id) );","title":"Schema"},{"location":"references/models/","text":"Models \u00b6 Models are an artifact of calls to pgml.train . See training for ways to create new models. Schema \u00b6 pgml.models 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 pgml . models ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , snapshot_id BIGINT NOT NULL , algorithm_name TEXT NOT NULL , hyperparams JSONB NOT NULL , status TEXT NOT NULL , search TEXT , search_params JSONB NOT NULL , search_args JSONB NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), metrics JSONB , pickle BYTEA , CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ), CONSTRAINT snapshot_id_fk FOREIGN KEY ( snapshot_id ) REFERENCES pgml . snapshots ( id ) );","title":"Models"},{"location":"references/models/#models","text":"Models are an artifact of calls to pgml.train . See training for ways to create new models.","title":"Models"},{"location":"references/models/#schema","text":"pgml.models 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 pgml . models ( id BIGSERIAL PRIMARY KEY , project_id BIGINT NOT NULL , snapshot_id BIGINT NOT NULL , algorithm_name TEXT NOT NULL , hyperparams JSONB NOT NULL , status TEXT NOT NULL , search TEXT , search_params JSONB NOT NULL , search_args JSONB NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), metrics JSONB , pickle BYTEA , CONSTRAINT project_id_fk FOREIGN KEY ( project_id ) REFERENCES pgml . projects ( id ), CONSTRAINT snapshot_id_fk FOREIGN KEY ( snapshot_id ) REFERENCES pgml . snapshots ( id ) );","title":"Schema"},{"location":"references/projects/","text":"Projects \u00b6 Projects are an artifact of calls to pgml.train . See training for ways to create new projects. Schema \u00b6 pgml.projects 1 2 3 4 5 6 7 pgml . projects ( id BIGSERIAL PRIMARY KEY , name TEXT NOT NULL , objective TEXT NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Projects"},{"location":"references/projects/#projects","text":"Projects are an artifact of calls to pgml.train . See training for ways to create new projects.","title":"Projects"},{"location":"references/projects/#schema","text":"pgml.projects 1 2 3 4 5 6 7 pgml . projects ( id BIGSERIAL PRIMARY KEY , name TEXT NOT NULL , objective TEXT NOT NULL , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Schema"},{"location":"references/snapshots/","text":"Snapshots \u00b6 Snapshots are an artifact of calls to pgml.train that specify the relation_name. See training for ways to create new snapshots. Schema \u00b6 pgml.snapshots 1 2 3 4 5 6 7 8 9 10 11 12 pgml . snapshots ( id BIGSERIAL PRIMARY KEY , relation_name TEXT NOT NULL , y_column_name TEXT [] NOT NULL , test_size FLOAT4 NOT NULL , test_sampling TEXT NOT NULL , status TEXT NOT NULL , columns JSONB , analysis JSONB , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () ); Tables \u00b6 Every snapshot has an accompaning table in the pgml schema. For example, the Snapshot with id = 42 has all data recorded in the table pgml.snaphot_42 . If the test_sampling was random for the training, the rows in the table were ORDER BY random() when it was created so that future samples can be consistently and efficiently randomized.","title":"Snapshots"},{"location":"references/snapshots/#snapshots","text":"Snapshots are an artifact of calls to pgml.train that specify the relation_name. See training for ways to create new snapshots.","title":"Snapshots"},{"location":"references/snapshots/#schema","text":"pgml.snapshots 1 2 3 4 5 6 7 8 9 10 11 12 pgml . snapshots ( id BIGSERIAL PRIMARY KEY , relation_name TEXT NOT NULL , y_column_name TEXT [] NOT NULL , test_size FLOAT4 NOT NULL , test_sampling TEXT NOT NULL , status TEXT NOT NULL , columns JSONB , analysis JSONB , created_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp (), updated_at TIMESTAMP WITHOUT TIME ZONE NOT NULL DEFAULT clock_timestamp () );","title":"Schema"},{"location":"references/snapshots/#tables","text":"Every snapshot has an accompaning table in the pgml schema. For example, the Snapshot with id = 42 has all data recorded in the table pgml.snaphot_42 . If the test_sampling was random for the training, the rows in the table were ORDER BY random() when it was created so that future samples can be consistently and efficiently randomized.","title":"Tables"}]}